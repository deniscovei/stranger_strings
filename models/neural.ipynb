{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffda5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.20.0-cp312-cp312-win_amd64.whl.metadata (4.6 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Using cached absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Using cached flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting opt_einsum>=2.3.2 (from tensorflow)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\marina\\anaconda3\\lib\\site-packages (from tensorflow) (24.1)\n",
      "Collecting protobuf>=5.28.0 (from tensorflow)\n",
      "  Using cached protobuf-6.33.0-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\marina\\anaconda3\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\marina\\anaconda3\\lib\\site-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\marina\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Using cached termcolor-3.2.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\marina\\anaconda3\\lib\\site-packages (from tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\marina\\anaconda3\\lib\\site-packages (from tensorflow) (1.14.1)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.76.0-cp312-cp312-win_amd64.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
      "  Using cached tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.10.0 (from tensorflow)\n",
      "  Using cached keras-3.12.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\marina\\anaconda3\\lib\\site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\marina\\anaconda3\\lib\\site-packages (from tensorflow) (3.11.0)\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.5.3-cp312-cp312-win_amd64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\marina\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Collecting typing_extensions>=3.6.6 (from tensorflow)\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: rich in c:\\users\\marina\\anaconda3\\lib\\site-packages (from keras>=3.10.0->tensorflow) (13.7.1)\n",
      "Collecting namex (from keras>=3.10.0->tensorflow)\n",
      "  Using cached namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.10.0->tensorflow)\n",
      "  Downloading optree-0.17.0-cp312-cp312-win_amd64.whl.metadata (34 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\marina\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\marina\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\marina\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\marina\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\marina\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\marina\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (10.4.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\marina\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\marina\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\marina\\anaconda3\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\marina\\anaconda3\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\marina\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Downloading tensorflow-2.20.0-cp312-cp312-win_amd64.whl (331.9 MB)\n",
      "   ---------------------------------------- 0.0/331.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 6.6/331.9 MB 36.6 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 12.8/331.9 MB 32.3 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 21.0/331.9 MB 34.0 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 30.4/331.9 MB 37.1 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 40.4/331.9 MB 38.9 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 46.9/331.9 MB 37.3 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 54.8/331.9 MB 37.9 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 65.3/331.9 MB 38.9 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 75.8/331.9 MB 40.3 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 85.2/331.9 MB 40.9 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 94.4/331.9 MB 41.3 MB/s eta 0:00:06\n",
      "   ------------ -------------------------- 102.2/331.9 MB 40.8 MB/s eta 0:00:06\n",
      "   ------------- ------------------------- 111.4/331.9 MB 40.9 MB/s eta 0:00:06\n",
      "   -------------- ------------------------ 119.5/331.9 MB 40.8 MB/s eta 0:00:06\n",
      "   --------------- ----------------------- 127.7/331.9 MB 40.8 MB/s eta 0:00:06\n",
      "   --------------- ----------------------- 135.8/331.9 MB 40.5 MB/s eta 0:00:05\n",
      "   ---------------- ---------------------- 143.1/331.9 MB 40.5 MB/s eta 0:00:05\n",
      "   ----------------- --------------------- 150.5/331.9 MB 39.9 MB/s eta 0:00:05\n",
      "   ------------------ -------------------- 159.1/331.9 MB 40.0 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 166.2/331.9 MB 39.8 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 173.5/331.9 MB 39.6 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 182.2/331.9 MB 39.5 MB/s eta 0:00:04\n",
      "   ---------------------- ---------------- 188.0/331.9 MB 39.0 MB/s eta 0:00:04\n",
      "   ---------------------- ---------------- 195.6/331.9 MB 38.9 MB/s eta 0:00:04\n",
      "   ----------------------- --------------- 203.7/331.9 MB 39.0 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 210.8/331.9 MB 38.8 MB/s eta 0:00:04\n",
      "   ------------------------- ------------- 218.9/331.9 MB 38.8 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 228.3/331.9 MB 38.9 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 232.8/331.9 MB 39.0 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 238.0/331.9 MB 37.8 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 246.4/331.9 MB 37.8 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 255.3/331.9 MB 37.9 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 262.9/331.9 MB 37.9 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 272.4/331.9 MB 38.3 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 281.0/331.9 MB 38.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 290.5/331.9 MB 38.3 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 299.9/331.9 MB 38.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 309.6/331.9 MB 38.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 318.5/331.9 MB 38.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  324.8/331.9 MB 38.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  331.9/331.9 MB 38.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  331.9/331.9 MB 38.2 MB/s eta 0:00:01\n",
      "   --------------------------------------- 331.9/331.9 MB 35.9 MB/s eta 0:00:00\n",
      "Using cached absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
      "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.76.0-cp312-cp312-win_amd64.whl (4.7 MB)\n",
      "   ---------------------------------------- 0.0/4.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 4.7/4.7 MB 31.5 MB/s eta 0:00:00\n",
      "Using cached keras-3.12.0-py3-none-any.whl (1.5 MB)\n",
      "Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "Downloading ml_dtypes-0.5.3-cp312-cp312-win_amd64.whl (208 kB)\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Using cached protobuf-6.33.0-cp310-abi3-win_amd64.whl (436 kB)\n",
      "Using cached tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached termcolor-3.2.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.17.0-cp312-cp312-win_amd64.whl (314 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, typing_extensions, termcolor, tensorboard-data-server, protobuf, opt_einsum, ml_dtypes, google_pasta, gast, astunparse, absl-py, optree, grpcio, tensorboard, keras, tensorflow\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.11.0\n",
      "    Uninstalling typing_extensions-4.11.0:\n",
      "      Successfully uninstalled typing_extensions-4.11.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.3\n",
      "    Uninstalling protobuf-4.25.3:\n",
      "      Successfully uninstalled protobuf-4.25.3\n",
      "Successfully installed absl-py-2.3.1 astunparse-1.6.3 flatbuffers-25.9.23 gast-0.6.0 google_pasta-0.2.0 grpcio-1.76.0 keras-3.12.0 libclang-18.1.1 ml_dtypes-0.5.3 namex-0.1.0 opt_einsum-3.4.0 optree-0.17.0 protobuf-6.33.0 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.2.0 typing_extensions-4.15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit 1.37.1 requires protobuf<6,>=3.20, but you have protobuf 6.33.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2345c6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Preprocessing utilities for fraud detection\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def load_data(csv_path='dataset/transactions.csv'):\n",
    "    \"\"\"Load and perform initial data cleaning\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print('Loaded dataset with shape:', df.shape)\n",
    "    \n",
    "    if csv_path == '../dataset/resampled_data.csv':\n",
    "        df = df.dropna()\n",
    "        df['target'] = df['target'].map({False: 0, True: 1})\n",
    "        df['isFraud'] = df['target']\n",
    "        df.drop([\"enteredCVV\", \"creditLimit\", \"noacqCountry\", \n",
    "                 \"acqCountry_CAN\", \"acqCountry_MEX\", \"acqCountry_PR\",\n",
    "                 \"acqCountry_US\", \"target\"], \n",
    "                 axis=1, inplace=True)\n",
    "        return df\n",
    "\n",
    "    # Drop all null columns\n",
    "    columns_to_drop = [\n",
    "        \"Unnamed: 0\", \"enteredCVV\", \"creditLimit\", \n",
    "        \"acqCountry\",\"customerId\", \"echoBuffer\", \n",
    "        \"merchantCity\", \"merchantState\", \"merchantZip\", \n",
    "        \"posOnPremises\", \"recurringAuthInd\"\n",
    "    ]\n",
    "    df = df.drop(columns_to_drop, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def one_hot_encode_categorical(df):\n",
    "    \"\"\"Apply one-hot encoding to categorical columns\"\"\"\n",
    "    print('Starting one-hot encoding...\\n')\n",
    "    \n",
    "    columns_with_nulls = ['acqCountry', 'merchantCountryCode', 'transactionType']\n",
    "    columns_without_nulls = ['merchantCategoryCode']\n",
    "    all_encode_columns = columns_with_nulls + columns_without_nulls\n",
    "    \n",
    "    # Handle columns with nulls - create indicator columns\n",
    "    for col in columns_with_nulls:\n",
    "        if col in df.columns:\n",
    "            null_indicator_col = f'no{col}'\n",
    "            df[null_indicator_col] = df[col].isnull().astype(int)\n",
    "            df[col] = df[col].fillna('MISSING')\n",
    "    \n",
    "    # Perform one-hot encoding\n",
    "    encoded_dfs = []\n",
    "    for col in all_encode_columns:\n",
    "        if col in df.columns:\n",
    "            one_hot = pd.get_dummies(df[col], prefix=col, drop_first=False)\n",
    "            \n",
    "            if col in columns_with_nulls:\n",
    "                missing_col_name = f'{col}_MISSING'\n",
    "                if missing_col_name in one_hot.columns:\n",
    "                    one_hot = one_hot.drop(columns=[missing_col_name])\n",
    "            \n",
    "            encoded_dfs.append(one_hot)\n",
    "            df = df.drop(columns=[col])\n",
    "    \n",
    "    if encoded_dfs:\n",
    "        df = pd.concat([df] + encoded_dfs, axis=1)\n",
    "    \n",
    "    print(f'Encoding complete! New shape: {df.shape}\\n')\n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_dates_to_numeric(df):\n",
    "    \"\"\"Convert date columns to days difference\"\"\"\n",
    "    print('Converting date columns to numeric features...\\n')\n",
    "    \n",
    "    date_columns = {\n",
    "        'currentExpDate': 'daysToCurrentExpDate',\n",
    "        'accountOpenDate': 'daysSinceAccountOpen',\n",
    "        'dateOfLastAddressChange': 'daysSinceLastAddressChange'\n",
    "    }\n",
    "    \n",
    "    df['transactionDateTime'] = pd.to_datetime(df['transactionDateTime'], errors='coerce')\n",
    "    \n",
    "    for original_col, new_col in date_columns.items():\n",
    "        if original_col in df.columns:\n",
    "            df[original_col] = pd.to_datetime(df[original_col], errors='coerce')\n",
    "            df[new_col] = (df['transactionDateTime'] - df[original_col]).dt.days\n",
    "            \n",
    "            if new_col == \"daysToCurrentExpDate\":\n",
    "                df[new_col] = -df[new_col]\n",
    "            \n",
    "            df = df.drop(columns=[original_col])\n",
    "    \n",
    "    df.drop(['transactionDateTime'], axis=1, inplace=True)\n",
    "    print('Date conversion complete!\\n')\n",
    "    return df\n",
    "\n",
    "\n",
    "def ordinal_encode_merchant(df):\n",
    "    \"\"\"Apply ordinal encoding to merchantName based on fraud probability\"\"\"\n",
    "    print('Applying ordinal encoding to merchantName...\\n')\n",
    "    \n",
    "    if 'merchantName' not in df.columns:\n",
    "        print('merchantName column not found - skipping')\n",
    "        return df\n",
    "    \n",
    "    merchant_stats = df.groupby('merchantName').agg({\n",
    "        'isFraud': ['sum', 'count']\n",
    "    }).reset_index()\n",
    "    \n",
    "    merchant_stats.columns = ['merchantName', 'fraud_count', 'total_count']\n",
    "    merchant_stats['prob_fraud'] = merchant_stats['fraud_count'] / merchant_stats['total_count']\n",
    "    merchant_stats['score'] = merchant_stats['prob_fraud']\n",
    "    merchant_stats = merchant_stats.sort_values('score', ascending=True).reset_index(drop=True)\n",
    "    merchant_stats['ordinal_rank'] = range(len(merchant_stats))\n",
    "    \n",
    "    merchant_to_rank = dict(zip(merchant_stats['merchantName'], merchant_stats['ordinal_rank']))\n",
    "    df['merchantName_ordinal'] = df['merchantName'].map(merchant_to_rank)\n",
    "    \n",
    "    unmapped_count = df['merchantName_ordinal'].isnull().sum()\n",
    "    if unmapped_count > 0:\n",
    "        median_rank = merchant_stats['ordinal_rank'].median()\n",
    "        df['merchantName_ordinal'].fillna(median_rank, inplace=True)\n",
    "    \n",
    "    df = df.drop(columns=['merchantName'])\n",
    "    print(f'Ordinal encoding complete! Total merchants: {len(merchant_stats)}\\n')\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_train_test_split(df, test_size=0.2, random_state=42):\n",
    "    \"\"\"Prepare X, y and create stratified train/test split\"\"\"\n",
    "    if 'isFraud' not in df.columns:\n",
    "        raise KeyError(\"Column 'isFraud' not found in dataframe\")\n",
    "    \n",
    "    y = df['isFraud']\n",
    "    X = df.drop(columns=['isFraud'])\n",
    "    \n",
    "    print(f'X shape: {X.shape}')\n",
    "    print(f'y shape: {y.shape}')\n",
    "    print(f'Fraud rate: {y.mean():.4f}\\n')\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, stratify=y, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    print(f'Train shapes -> X: {X_train.shape}, y: {y_train.shape}')\n",
    "    print(f'Test shapes  -> X: {X_test.shape}, y: {y_test.shape}')\n",
    "    print(f'Train fraud rate: {y_train.mean():.4f}')\n",
    "    print(f'Test fraud rate: {y_test.mean():.4f}\\n')\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def preprocess_pipeline(csv_path='../dataset/transactions.csv'):\n",
    "    \"\"\"Full preprocessing pipeline\"\"\"\n",
    "    print('='*60)\n",
    "    print('STARTING PREPROCESSING PIPELINE')\n",
    "    print('='*60 + '\\n')\n",
    "    \n",
    "    # Load data\n",
    "    df = load_data(csv_path)\n",
    "\n",
    "    if csv_path != '../dataset/resampled_data.csv':\n",
    "        # One-hot encoding\n",
    "        df = one_hot_encode_categorical(df)\n",
    "        \n",
    "        # Date conversion\n",
    "        df = convert_dates_to_numeric(df)\n",
    "        \n",
    "        # Merchant encoding\n",
    "        df = ordinal_encode_merchant(df)\n",
    "    \n",
    "    # 2. Preprocesare (scalare)\n",
    "    # scaler = StandardScaler().set_output(transform=\"pandas\")\n",
    "    # df_scaled = scaler.fit_transform(df)\n",
    "\n",
    "    df = df.dropna()\n",
    "\n",
    "    df_scaled = df / df.max()\n",
    "    df_scaled = df_scaled.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = prepare_train_test_split(df_scaled)\n",
    "\n",
    "    print('='*60)\n",
    "    print('PREPROCESSING COMPLETE')\n",
    "    print('='*60 + '\\n')\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ed9315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING PREPROCESSING PIPELINE\n",
      "============================================================\n",
      "\n",
      "Loaded dataset with shape: (786363, 30)\n",
      "Starting one-hot encoding...\n",
      "\n",
      "Encoding complete! New shape: (786363, 44)\n",
      "\n",
      "Converting date columns to numeric features...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marina\\AppData\\Local\\Temp\\ipykernel_27616\\3504367928.py:87: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[original_col] = pd.to_datetime(df[original_col], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date conversion complete!\n",
      "\n",
      "Applying ordinal encoding to merchantName...\n",
      "\n",
      "Ordinal encoding complete! Total merchants: 2490\n",
      "\n",
      "X shape: (781903, 42)\n",
      "y shape: (781903,)\n",
      "Fraud rate: 0.0155\n",
      "\n",
      "Train shapes -> X: (625522, 42), y: (625522,)\n",
      "Test shapes  -> X: (156381, 42), y: (156381,)\n",
      "Train fraud rate: 0.0155\n",
      "Test fraud rate: 0.0155\n",
      "\n",
      "============================================================\n",
      "PREPROCESSING COMPLETE\n",
      "============================================================\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 625522 entries, 509005 to 259779\n",
      "Data columns (total 42 columns):\n",
      " #   Column                                     Non-Null Count   Dtype  \n",
      "---  ------                                     --------------   -----  \n",
      " 0   accountNumber                              625522 non-null  float64\n",
      " 1   availableMoney                             625522 non-null  float64\n",
      " 2   transactionAmount                          625522 non-null  float64\n",
      " 3   posEntryMode                               625522 non-null  float64\n",
      " 4   posConditionCode                           625522 non-null  float64\n",
      " 5   cardCVV                                    625522 non-null  float64\n",
      " 6   cardLast4Digits                            625522 non-null  float64\n",
      " 7   currentBalance                             625522 non-null  float64\n",
      " 8   cardPresent                                625522 non-null  float64\n",
      " 9   expirationDateKeyInMatch                   625522 non-null  float64\n",
      " 10  nomerchantCountryCode                      625522 non-null  float64\n",
      " 11  notransactionType                          625522 non-null  float64\n",
      " 12  merchantCountryCode_CAN                    625522 non-null  float64\n",
      " 13  merchantCountryCode_MEX                    625522 non-null  float64\n",
      " 14  merchantCountryCode_PR                     625522 non-null  float64\n",
      " 15  merchantCountryCode_US                     625522 non-null  float64\n",
      " 16  transactionType_ADDRESS_VERIFICATION       625522 non-null  float64\n",
      " 17  transactionType_PURCHASE                   625522 non-null  float64\n",
      " 18  transactionType_REVERSAL                   625522 non-null  float64\n",
      " 19  merchantCategoryCode_airline               625522 non-null  float64\n",
      " 20  merchantCategoryCode_auto                  625522 non-null  float64\n",
      " 21  merchantCategoryCode_cable/phone           625522 non-null  float64\n",
      " 22  merchantCategoryCode_entertainment         625522 non-null  float64\n",
      " 23  merchantCategoryCode_fastfood              625522 non-null  float64\n",
      " 24  merchantCategoryCode_food                  625522 non-null  float64\n",
      " 25  merchantCategoryCode_food_delivery         625522 non-null  float64\n",
      " 26  merchantCategoryCode_fuel                  625522 non-null  float64\n",
      " 27  merchantCategoryCode_furniture             625522 non-null  float64\n",
      " 28  merchantCategoryCode_gym                   625522 non-null  float64\n",
      " 29  merchantCategoryCode_health                625522 non-null  float64\n",
      " 30  merchantCategoryCode_hotels                625522 non-null  float64\n",
      " 31  merchantCategoryCode_mobileapps            625522 non-null  float64\n",
      " 32  merchantCategoryCode_online_gifts          625522 non-null  float64\n",
      " 33  merchantCategoryCode_online_retail         625522 non-null  float64\n",
      " 34  merchantCategoryCode_online_subscriptions  625522 non-null  float64\n",
      " 35  merchantCategoryCode_personal care         625522 non-null  float64\n",
      " 36  merchantCategoryCode_rideshare             625522 non-null  float64\n",
      " 37  merchantCategoryCode_subscriptions         625522 non-null  float64\n",
      " 38  daysToCurrentExpDate                       625522 non-null  float64\n",
      " 39  daysSinceAccountOpen                       625522 non-null  float64\n",
      " 40  daysSinceLastAddressChange                 625522 non-null  float64\n",
      " 41  merchantName_ordinal                       625522 non-null  float64\n",
      "dtypes: float64(42)\n",
      "memory usage: 205.2 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score, roc_auc_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "X_train, X_test, y_train, y_test, df = preprocess_pipeline('../dataset/transactions.csv')\n",
    "aux = y_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55820065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# from scipy.stats import pointbiserialr\n",
    "\n",
    "# df['isFraud'] = df['isFraud'].astype('int64')\n",
    "# numerical_features = df.select_dtypes(include=['float64', 'int64']).columns.drop('isFraud')\n",
    "\n",
    "# # 1. Plot istograme pentru fiecare feature numeric, diferențiat pe fraud / non-fraud:\n",
    "# for col in numerical_features:\n",
    "#     plt.figure(figsize=(8, 4))\n",
    "#     sns.histplot(data=df, x=col, hue=\"isFraud\", bins=50, kde=True, stat=\"density\", element=\"step\")\n",
    "#     plt.title(f'Distribuție {col} pe fraude vs non-fraude')\n",
    "#     plt.show()\n",
    "\n",
    "# # 2. Heatmap corelații între variabile numerice și target\n",
    "# correlations = {}\n",
    "# for col in numerical_features:\n",
    "#     corr, _ = pointbiserialr(df[col].fillna(0), df['isFraud'])\n",
    "#     correlations[col] = corr\n",
    "\n",
    "# corr_df = pd.DataFrame.from_dict(correlations, orient='index', columns=['corr_with_target']).sort_values(by='corr_with_target', ascending=False)\n",
    "# plt.figure(figsize=(6, len(corr_df)//2))\n",
    "# sns.heatmap(corr_df, annot=True, cmap='coolwarm')\n",
    "# plt.title('Corelația fiecărei variabile cu targetul')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9d96e637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marina\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4887/4887\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - Precision: 0.0166 - Recall: 0.0234 - auc_30: 0.5650 - loss: 0.1332 - val_Precision: 0.0000e+00 - val_Recall: 0.0000e+00 - val_auc_30: 0.7125 - val_loss: 0.0750\n",
      "Epoch 2/10\n",
      "\u001b[1m4887/4887\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - Precision: 0.0000e+00 - Recall: 0.0000e+00 - auc_30: 0.6878 - loss: 0.0764 - val_Precision: 0.0000e+00 - val_Recall: 0.0000e+00 - val_auc_30: 0.7563 - val_loss: 0.0729\n",
      "Epoch 3/10\n",
      "\u001b[1m4887/4887\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - Precision: 0.5000 - Recall: 1.0307e-04 - auc_30: 0.7210 - loss: 0.0749 - val_Precision: 0.0000e+00 - val_Recall: 0.0000e+00 - val_auc_30: 0.7574 - val_loss: 0.0726\n",
      "Epoch 4/10\n",
      "\u001b[1m4887/4887\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - Precision: 0.0000e+00 - Recall: 0.0000e+00 - auc_30: 0.7307 - loss: 0.0743 - val_Precision: 0.0000e+00 - val_Recall: 0.0000e+00 - val_auc_30: 0.7618 - val_loss: 0.0725\n",
      "Epoch 5/10\n",
      "\u001b[1m4887/4887\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - Precision: 0.0000e+00 - Recall: 0.0000e+00 - auc_30: 0.7392 - loss: 0.0739 - val_Precision: 0.0000e+00 - val_Recall: 0.0000e+00 - val_auc_30: 0.7665 - val_loss: 0.0722\n",
      "Epoch 6/10\n",
      "\u001b[1m4887/4887\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - Precision: 0.0000e+00 - Recall: 0.0000e+00 - auc_30: 0.7436 - loss: 0.0736 - val_Precision: 0.0000e+00 - val_Recall: 0.0000e+00 - val_auc_30: 0.7674 - val_loss: 0.0720\n",
      "Epoch 7/10\n",
      "\u001b[1m4887/4887\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - Precision: 0.0000e+00 - Recall: 0.0000e+00 - auc_30: 0.7481 - loss: 0.0733 - val_Precision: 0.0000e+00 - val_Recall: 0.0000e+00 - val_auc_30: 0.7703 - val_loss: 0.0718\n",
      "Epoch 8/10\n",
      "\u001b[1m4887/4887\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - Precision: 0.0000e+00 - Recall: 0.0000e+00 - auc_30: 0.7517 - loss: 0.0730 - val_Precision: 0.0000e+00 - val_Recall: 0.0000e+00 - val_auc_30: 0.7725 - val_loss: 0.0717\n",
      "Epoch 9/10\n",
      "\u001b[1m4887/4887\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - Precision: 0.0000e+00 - Recall: 0.0000e+00 - auc_30: 0.7550 - loss: 0.0729 - val_Precision: 0.0000e+00 - val_Recall: 0.0000e+00 - val_auc_30: 0.7760 - val_loss: 0.0716\n",
      "Epoch 10/10\n",
      "\u001b[1m4887/4887\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - Precision: 0.0000e+00 - Recall: 0.0000e+00 - auc_30: 0.7602 - loss: 0.0726 - val_Precision: 0.0000e+00 - val_Recall: 0.0000e+00 - val_auc_30: 0.7793 - val_loss: 0.0714\n",
      "\u001b[1m4887/4887\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 525us/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# def soft_f1_loss(y_true, y_pred, epsilon=1e-7):\n",
    "#     y_pred = tf.clip_by_value(y_pred, epsilon, 1 - epsilon)\n",
    "#     tp = tf.reduce_sum(y_true * y_pred)\n",
    "#     fp = tf.reduce_sum((1 - y_true) * y_pred)\n",
    "#     fn = tf.reduce_sum(y_true * (1 - y_pred))\n",
    "#     soft_f1 = 2 * tp / (2 * tp + fp + fn + epsilon)\n",
    "#     return 1 - soft_f1  # Vrem să maximizăm F1, deci minimizăm 1-F1\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.35),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.15),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "optimizer=keras.optimizers.Adam(learning_rate=0.0005)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['Recall', 'Precision', keras.metrics.AUC()]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    batch_size=128,\n",
    "    epochs=10,\n",
    ")\n",
    "# Predict with custom threshold\n",
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = (y_pred_proba >= 0.6).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a91f13b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "255354    0.0\n",
      "223774    0.0\n",
      "512020    0.0\n",
      "441847    0.0\n",
      "308933    0.0\n",
      "         ... \n",
      "174404    0.0\n",
      "636704    0.0\n",
      "15828     0.0\n",
      "684132    0.0\n",
      "118390    0.0\n",
      "Name: isFraud, Length: 156381, dtype: float64\n",
      "(156381,)\n",
      "(156381, 1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      1.00      0.99    153956\n",
      "         1.0       0.00      0.00      0.00      2425\n",
      "\n",
      "    accuracy                           0.98    156381\n",
      "   macro avg       0.49      0.50      0.50    156381\n",
      "weighted avg       0.97      0.98      0.98    156381\n",
      "\n",
      "F1-score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marina\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Marina\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Marina\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.0\n",
      "Precision: 0.0\n",
      "Accuracy: 0.9844930010679047\n",
      "AUC: 0.7823672850162304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marina\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(y_test.isna().sum())\n",
    "print(y_test)\n",
    "\n",
    "print(y_test.shape)\n",
    "print(y_pred.shape)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"AUC:\", roc_auc_score(y_test, y_pred_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f50479d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING PREPROCESSING PIPELINE\n",
      "============================================================\n",
      "\n",
      "Loaded dataset with shape: (786363, 30)\n",
      "Starting one-hot encoding...\n",
      "\n",
      "Encoding complete! New shape: (786363, 44)\n",
      "\n",
      "Converting date columns to numeric features...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marina\\AppData\\Local\\Temp\\ipykernel_27616\\2161341927.py:87: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[original_col] = pd.to_datetime(df[original_col], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date conversion complete!\n",
      "\n",
      "Applying ordinal encoding to merchantName...\n",
      "\n",
      "Ordinal encoding complete! Total merchants: 2490\n",
      "\n",
      "X shape: (786363, 42)\n",
      "y shape: (786363,)\n",
      "Fraud rate: 0.0158\n",
      "\n",
      "Train shapes -> X: (629090, 42), y: (629090,)\n",
      "Test shapes  -> X: (157273, 42), y: (157273,)\n",
      "Train fraud rate: 0.0158\n",
      "Test fraud rate: 0.0158\n",
      "\n",
      "============================================================\n",
      "PREPROCESSING COMPLETE\n",
      "============================================================\n",
      "\n",
      "\u001b[1m4915/4915\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 505us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.00      0.00      0.00    154790\n",
      "        True       0.02      1.00      0.03      2483\n",
      "\n",
      "    accuracy                           0.02    157273\n",
      "   macro avg       0.01      0.50      0.02    157273\n",
      "weighted avg       0.00      0.02      0.00    157273\n",
      "\n",
      "F1-score: 0.03108490447933098\n",
      "Recall: 1.0\n",
      "Precision: 0.015787833893929664\n",
      "Accuracy: 0.015787833893929664\n",
      "AUC: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marina\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Marina\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Marina\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, df = preprocess_pipeline('../dataset/transactions.csv')\n",
    "\n",
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"AUC:\", roc_auc_score(y_test, y_pred_proba))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
