{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8fa9678c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Preprocessing utilities for fraud detection\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def load_data(csv_path='dataset/transactions.csv'):\n",
    "    \"\"\"Load and perform initial data cleaning\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print('Loaded dataset with shape:', df.shape)\n",
    "    \n",
    "    if csv_path == '../dataset/resampled_data.csv':\n",
    "        df = df.dropna()\n",
    "        df['target'] = df['target'].map({False: 0, True: 1})\n",
    "        df['isFraud'] = df['target']\n",
    "        df.drop([\"enteredCVV\", \"creditLimit\", \"noacqCountry\", \n",
    "                 \"acqCountry_CAN\", \"acqCountry_MEX\", \"acqCountry_PR\",\n",
    "                 \"acqCountry_US\", \"target\"], \n",
    "                 axis=1, inplace=True)\n",
    "        return df\n",
    "\n",
    "    # Drop all null columns\n",
    "    columns_to_drop = [\n",
    "        \"Unnamed: 0\", \"enteredCVV\", \"creditLimit\", \n",
    "        \"acqCountry\",\"customerId\", \"echoBuffer\", \n",
    "        \"merchantCity\", \"merchantState\", \"merchantZip\", \n",
    "        \"posOnPremises\", \"recurringAuthInd\"\n",
    "    ]\n",
    "    df = df.drop(columns_to_drop, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def one_hot_encode_categorical(df):\n",
    "    \"\"\"Apply one-hot encoding to categorical columns\"\"\"\n",
    "    print('Starting one-hot encoding...\\n')\n",
    "    \n",
    "    columns_with_nulls = ['acqCountry', 'merchantCountryCode', 'transactionType']\n",
    "    columns_without_nulls = ['merchantCategoryCode']\n",
    "    all_encode_columns = columns_with_nulls + columns_without_nulls\n",
    "    \n",
    "    # Handle columns with nulls - create indicator columns\n",
    "    for col in columns_with_nulls:\n",
    "        if col in df.columns:\n",
    "            null_indicator_col = f'no{col}'\n",
    "            df[null_indicator_col] = df[col].isnull().astype(int)\n",
    "            df[col] = df[col].fillna('MISSING')\n",
    "    \n",
    "    # Perform one-hot encoding\n",
    "    encoded_dfs = []\n",
    "    for col in all_encode_columns:\n",
    "        if col in df.columns:\n",
    "            one_hot = pd.get_dummies(df[col], prefix=col, drop_first=False)\n",
    "            \n",
    "            if col in columns_with_nulls:\n",
    "                missing_col_name = f'{col}_MISSING'\n",
    "                if missing_col_name in one_hot.columns:\n",
    "                    one_hot = one_hot.drop(columns=[missing_col_name])\n",
    "            \n",
    "            encoded_dfs.append(one_hot)\n",
    "            df = df.drop(columns=[col])\n",
    "    \n",
    "    if encoded_dfs:\n",
    "        df = pd.concat([df] + encoded_dfs, axis=1)\n",
    "    \n",
    "    print(f'Encoding complete! New shape: {df.shape}\\n')\n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_dates_to_numeric(df):\n",
    "    \"\"\"Convert date columns to days difference\"\"\"\n",
    "    print('Converting date columns to numeric features...\\n')\n",
    "    \n",
    "    date_columns = {\n",
    "        'currentExpDate': 'daysToCurrentExpDate',\n",
    "        'accountOpenDate': 'daysSinceAccountOpen',\n",
    "        'dateOfLastAddressChange': 'daysSinceLastAddressChange'\n",
    "    }\n",
    "    \n",
    "    df['transactionDateTime'] = pd.to_datetime(df['transactionDateTime'], errors='coerce')\n",
    "    \n",
    "    for original_col, new_col in date_columns.items():\n",
    "        if original_col in df.columns:\n",
    "            df[original_col] = pd.to_datetime(df[original_col], errors='coerce')\n",
    "            df[new_col] = (df['transactionDateTime'] - df[original_col]).dt.days\n",
    "            \n",
    "            if new_col == \"daysToCurrentExpDate\":\n",
    "                df[new_col] = -df[new_col]\n",
    "            \n",
    "            df = df.drop(columns=[original_col])\n",
    "    \n",
    "    df.drop(['transactionDateTime'], axis=1, inplace=True)\n",
    "    print('Date conversion complete!\\n')\n",
    "    return df\n",
    "\n",
    "\n",
    "def ordinal_encode_merchant(df):\n",
    "    \"\"\"Apply ordinal encoding to merchantName based on fraud probability\"\"\"\n",
    "    print('Applying ordinal encoding to merchantName...\\n')\n",
    "    \n",
    "    if 'merchantName' not in df.columns:\n",
    "        print('merchantName column not found - skipping')\n",
    "        return df\n",
    "    \n",
    "    merchant_stats = df.groupby('merchantName').agg({\n",
    "        'isFraud': ['sum', 'count']\n",
    "    }).reset_index()\n",
    "    \n",
    "    merchant_stats.columns = ['merchantName', 'fraud_count', 'total_count']\n",
    "    merchant_stats['prob_fraud'] = merchant_stats['fraud_count'] / merchant_stats['total_count']\n",
    "    merchant_stats['score'] = merchant_stats['prob_fraud']\n",
    "    merchant_stats = merchant_stats.sort_values('score', ascending=True).reset_index(drop=True)\n",
    "    merchant_stats['ordinal_rank'] = range(len(merchant_stats))\n",
    "    \n",
    "    merchant_to_rank = dict(zip(merchant_stats['merchantName'], merchant_stats['ordinal_rank']))\n",
    "    df['merchantName_ordinal'] = df['merchantName'].map(merchant_to_rank)\n",
    "    \n",
    "    unmapped_count = df['merchantName_ordinal'].isnull().sum()\n",
    "    if unmapped_count > 0:\n",
    "        median_rank = merchant_stats['ordinal_rank'].median()\n",
    "        df['merchantName_ordinal'].fillna(median_rank, inplace=True)\n",
    "    \n",
    "    df = df.drop(columns=['merchantName'])\n",
    "    print(f'Ordinal encoding complete! Total merchants: {len(merchant_stats)}\\n')\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_train_test_split(df, test_size=0.2, random_state=42):\n",
    "    \"\"\"Prepare X, y and create stratified train/test split\"\"\"\n",
    "    if 'isFraud' not in df.columns:\n",
    "        raise KeyError(\"Column 'isFraud' not found in dataframe\")\n",
    "    \n",
    "    y = df['isFraud']\n",
    "    X = df.drop(columns=['isFraud'])\n",
    "    \n",
    "    print(f'X shape: {X.shape}')\n",
    "    print(f'y shape: {y.shape}')\n",
    "    print(f'Fraud rate: {y.mean():.4f}\\n')\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, stratify=y, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    print(f'Train shapes -> X: {X_train.shape}, y: {y_train.shape}')\n",
    "    print(f'Test shapes  -> X: {X_test.shape}, y: {y_test.shape}')\n",
    "    print(f'Train fraud rate: {y_train.mean():.4f}')\n",
    "    print(f'Test fraud rate: {y_test.mean():.4f}\\n')\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def preprocess_pipeline(csv_path='../dataset/transactions.csv'):\n",
    "    \"\"\"Full preprocessing pipeline\"\"\"\n",
    "    print('='*60)\n",
    "    print('STARTING PREPROCESSING PIPELINE')\n",
    "    print('='*60 + '\\n')\n",
    "    \n",
    "    # Load data\n",
    "    df = load_data(csv_path)\n",
    "\n",
    "    if csv_path != '../dataset/resampled_data.csv':\n",
    "        # One-hot encoding\n",
    "        df = one_hot_encode_categorical(df)\n",
    "        \n",
    "        # Date conversion\n",
    "        df = convert_dates_to_numeric(df)\n",
    "        \n",
    "        # Merchant encoding\n",
    "        df = ordinal_encode_merchant(df)\n",
    "    \n",
    "    # 2. Preprocesare (scalare)\n",
    "    # scaler = StandardScaler().set_output(transform=\"pandas\")\n",
    "    # df_scaled = scaler.fit_transform(df)\n",
    "\n",
    "    df = df.dropna()\n",
    "\n",
    "    df_scaled = df / df.max()\n",
    "    df_scaled = df_scaled.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = prepare_train_test_split(df_scaled)\n",
    "\n",
    "    print('='*60)\n",
    "    print('PREPROCESSING COMPLETE')\n",
    "    print('='*60 + '\\n')\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "74c12cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING PREPROCESSING PIPELINE\n",
      "============================================================\n",
      "\n",
      "Loaded dataset with shape: (786363, 30)\n",
      "Starting one-hot encoding...\n",
      "\n",
      "Encoding complete! New shape: (786363, 44)\n",
      "\n",
      "Converting date columns to numeric features...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marina\\AppData\\Local\\Temp\\ipykernel_3560\\3504367928.py:87: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[original_col] = pd.to_datetime(df[original_col], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date conversion complete!\n",
      "\n",
      "Applying ordinal encoding to merchantName...\n",
      "\n",
      "Ordinal encoding complete! Total merchants: 2490\n",
      "\n",
      "X shape: (781903, 42)\n",
      "y shape: (781903,)\n",
      "Fraud rate: 0.0155\n",
      "\n",
      "Train shapes -> X: (625522, 42), y: (625522,)\n",
      "Test shapes  -> X: (156381, 42), y: (156381,)\n",
      "Train fraud rate: 0.0155\n",
      "Test fraud rate: 0.0155\n",
      "\n",
      "============================================================\n",
      "PREPROCESSING COMPLETE\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, df = preprocess_pipeline('../dataset/transactions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23a63bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625522\n",
      "Counter({0.0: 562969, 1.0: 9702})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9845    1.0000    0.9922    153956\n",
      "         1.0     0.4000    0.0008    0.0016      2425\n",
      "\n",
      "    accuracy                         0.9845    156381\n",
      "   macro avg     0.6923    0.5004    0.4969    156381\n",
      "weighted avg     0.9754    0.9845    0.9768    156381\n",
      "\n",
      "Confusion Matrix:\n",
      " [[153953      3]\n",
      " [  2423      2]]\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# # from imblearn.under_sampling import ClusterCentroids\n",
    "\n",
    "# # cc = ClusterCentroids(random_state=42)\n",
    "# # X_resampled, y_resampled = cc.fit_resample(X_train, y_train)\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# total = y_train.shape[0]\n",
    "# print(total)\n",
    "# # Definește obiectul de undersampling; sampling_strategy=\"majority\" reduce doar clasa majoritară\n",
    "# undersample = RandomUnderSampler(sampling_strategy={0: int(0.9* total)}, random_state=42)\n",
    "\n",
    "# # Aplică pe date (X = feature-uri, y = etichetă)\n",
    "# X_resampled, y_resampled = undersample.fit_resample(X_train, y_train)\n",
    "\n",
    "# # Distribuția claselor după undersampling\n",
    "# from collections import Counter\n",
    "# print(Counter(y_resampled))\n",
    "\n",
    "# # Creează și antrenează modelul\n",
    "# model = LogisticRegression(max_iter=2000, random_state=42, C=0.2)\n",
    "# model.fit(X_resampled, y_resampled)\n",
    "\n",
    "# # Predicții\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# # Output metrici relevante\n",
    "# print(classification_report(y_test, y_pred, digits=4))\n",
    "# print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2e1887d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9866    0.9994    0.9930    153956\n",
      "         1.0     0.7770    0.1394    0.2364      2425\n",
      "\n",
      "    accuracy                         0.9860    156381\n",
      "   macro avg     0.8818    0.5694    0.6147    156381\n",
      "weighted avg     0.9834    0.9860    0.9812    156381\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Scaling obligatoriu pentru MLP!\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Definește modelele de bază\n",
    "base_learners = [\n",
    "    ('lgbm', lgb.LGBMClassifier(\n",
    "        objective='binary',\n",
    "        metric='f1',\n",
    "        boosting_type='gbdt',\n",
    "        n_estimators=800,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=8,\n",
    "        subsample=0.6,\n",
    "        subsample_freq=1,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.5,\n",
    "        reg_lambda=0.5,\n",
    "        # scale_pos_weight=scale_pos_weight_lgb,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1,\n",
    "        is_unbalance=True\n",
    "    )),\n",
    "    ('mlp', MLPClassifier(hidden_layer_sizes=(64, 32), activation='relu', solver='adam', \n",
    "                    max_iter=100, random_state=42)),\n",
    "    ('xgb', XGBClassifier(\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='aucpr',  # XGBoost doesn't support f1 directly, use logloss\n",
    "        scale_pos_weight=1,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=1000,\n",
    "        min_child_weight=1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        gamma=0,\n",
    "        reg_alpha=0,\n",
    "        reg_lambda=1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "]\n",
    "\n",
    "# Meta-modelul\n",
    "meta_learner = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Stacking\n",
    "stack = StackingClassifier(\n",
    "    estimators=base_learners,\n",
    "    final_estimator=meta_learner,\n",
    "    passthrough=False,           # dacă vrei ca meta-model să vadă și features-urile originale, pune True\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Antrenează\n",
    "stack.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluează\n",
    "y_pred = stack.predict(X_test_scaled)\n",
    "print(classification_report(y_test, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "797cd562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9862    0.9992    0.9927    153956\n",
      "         1.0     0.6830    0.1146    0.1963      2425\n",
      "\n",
      "    accuracy                         0.9854    156381\n",
      "   macro avg     0.8346    0.5569    0.5945    156381\n",
      "weighted avg     0.9815    0.9854    0.9803    156381\n",
      "\n",
      "Confusion Matrix:\n",
      " [[153827    129]\n",
      " [  2147    278]]\n"
     ]
    }
   ],
   "source": [
    "# X_train, X_test, y_train, y_test, df = preprocess_pipeline('../dataset/transactions.csv')\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Scaling obligatoriu pentru MLP!\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Creează și antrenează MLP - poți ajusta hidden_layer_sizes (ex: (64, 32)), max_iter etc\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(64, 32, 16), activation='relu', solver='adam', \n",
    "                    max_iter=100, random_state=42)\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predicții și metrici\n",
    "y_pred = mlp.predict(X_test_scaled)\n",
    "y_pred1 = y_pred.copy()\n",
    "\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "13d1f8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LightGBM model training and evaluation for fraud detection\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import (\n",
    "    classification_report, recall_score, precision_score, \n",
    "    f1_score, confusion_matrix, make_scorer\n",
    ")\n",
    "\n",
    "\n",
    "def train_lightgbm(X_train, y_train, verbose=True):\n",
    "    \"\"\"Train LightGBM model optimized for recall\"\"\"\n",
    "    if verbose:\n",
    "        print('='*60)\n",
    "        print('TRAINING LIGHTGBM - OPTIMIZED FOR RECALL')\n",
    "        print('='*60 + '\\n')\n",
    "    \n",
    "    # Calculate scale_pos_weight for class imbalance\n",
    "    neg_count = (y_train == 0).sum()\n",
    "    pos_count = (y_train == 1).sum()\n",
    "    scale_pos_weight_lgb = neg_count / pos_count if pos_count > 0 else 1\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Class imbalance ratio (neg/pos): {scale_pos_weight_lgb:.2f}')\n",
    "        print(f'Using scale_pos_weight={scale_pos_weight_lgb:.2f} to boost recall\\n')\n",
    "    \n",
    "    lgb_model = lgb.LGBMClassifier(\n",
    "        objective='binary',\n",
    "        metric='f1',\n",
    "        boosting_type='gbdt',\n",
    "        n_estimators=800,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=8,\n",
    "        subsample=0.6,\n",
    "        subsample_freq=1,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.5,\n",
    "        reg_lambda=0.5,\n",
    "        # scale_pos_weight=scale_pos_weight_lgb,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1,\n",
    "        is_unbalance=True\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print('Training LightGBM model...')\n",
    "    \n",
    "    lgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    if verbose:\n",
    "        print('Training complete!\\n')\n",
    "    \n",
    "    return lgb_model\n",
    "\n",
    "\n",
    "def evaluate_lightgbm(model, X_train, y_train, X_test, y_test, verbose=True):\n",
    "    \"\"\"Evaluate LightGBM model on train and test sets\"\"\"\n",
    "    if verbose:\n",
    "        print('='*60)\n",
    "        print('EVALUATING LIGHTGBM MODEL')\n",
    "        print('='*60 + '\\n')\n",
    "    \n",
    "    # Train set predictions\n",
    "    y_train_pred_lgb = model.predict(X_train)\n",
    "    \n",
    "    if verbose:\n",
    "        print('=== TRAIN SET PERFORMANCE ===')\n",
    "        print(f'Recall (fraud class): {recall_score(y_train, y_train_pred_lgb):.4f}')\n",
    "        print(f'Precision (fraud class): {precision_score(y_train, y_train_pred_lgb):.4f}')\n",
    "        print(f'F1-Score (fraud class): {f1_score(y_train, y_train_pred_lgb):.4f}')\n",
    "        print('\\nConfusion Matrix (Train):')\n",
    "        print(confusion_matrix(y_train, y_train_pred_lgb))\n",
    "    \n",
    "    # Test set predictions\n",
    "    y_test_pred_lgb = model.predict(X_test)\n",
    "    \n",
    "    if verbose:\n",
    "        print('\\n=== TEST SET PERFORMANCE ===')\n",
    "        print(f'Recall (fraud class): {recall_score(y_test, y_test_pred_lgb):.4f}')\n",
    "        print(f'Precision (fraud class): {precision_score(y_test, y_test_pred_lgb):.4f}')\n",
    "        print(f'F1-Score (fraud class): {f1_score(y_test, y_test_pred_lgb):.4f}')\n",
    "        print('\\nConfusion Matrix (Test):')\n",
    "        print(confusion_matrix(y_test, y_test_pred_lgb))\n",
    "        print('\\nClassification Report (Test):')\n",
    "        print(classification_report(y_test, y_test_pred_lgb))\n",
    "    \n",
    "    return y_train_pred_lgb, y_test_pred_lgb\n",
    "\n",
    "\n",
    "def get_feature_importance(model, X_train, top_n=15, verbose=True):\n",
    "    \"\"\"Get and display feature importances\"\"\"\n",
    "    if hasattr(X_train, 'columns'):\n",
    "        fi_lgb = pd.Series(model.feature_importances_, index=X_train.columns)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'\\nTop {top_n} Most Important Features (LightGBM):')\n",
    "            print(fi_lgb.nlargest(top_n))\n",
    "        \n",
    "        return fi_lgb\n",
    "    return None\n",
    "\n",
    "\n",
    "def train_lightgbm_with_random_search(X_train, y_train, n_iter=100, cv=5, verbose=True):\n",
    "    \"\"\"\n",
    "    Train LightGBM with RandomizedSearchCV for hyperparameter tuning\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        y_train: Training labels\n",
    "        n_iter: Number of parameter settings sampled (default: 100)\n",
    "        cv: Number of cross-validation folds (default: 5)\n",
    "        verbose: Whether to print progress\n",
    "    \n",
    "    Returns:\n",
    "        best_model: The best estimator found by RandomizedSearchCV\n",
    "        search_results: The RandomizedSearchCV object with all results\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print('='*60)\n",
    "        print('LIGHTGBM RANDOMIZED SEARCH CV')\n",
    "        print('='*60 + '\\n')\n",
    "        print(f'Configuration:')\n",
    "        print(f'  n_iter: {n_iter} (parameter combinations)')\n",
    "        print(f'  cv: {cv} (cross-validation folds)')\n",
    "        print(f'  Total fits: {n_iter * cv}\\n')\n",
    "    \n",
    "    # Calculate scale_pos_weight for class imbalance\n",
    "    neg_count = (y_train == 0).sum()\n",
    "    pos_count = (y_train == 1).sum()\n",
    "    scale_pos_weight_lgb = neg_count / pos_count if pos_count > 0 else 1\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Class imbalance ratio: {scale_pos_weight_lgb:.2f}\\n')\n",
    "    \n",
    "    # Define parameter distributions for random search\n",
    "    param_distributions = {\n",
    "        'n_estimators': [100, 150, 200, 250, 300, 350, 400],\n",
    "        'learning_rate': [0.01, 0.03, 0.05, 0.07, 0.1, 0.15],\n",
    "        'max_depth': [6, 8, 10, 12, 15, -1],\n",
    "        'num_leaves': [31, 50, 63, 80, 100, 127],\n",
    "        'min_child_samples': [10, 15, 20, 25, 30, 40, 50],\n",
    "        'min_child_weight': [0.001, 0.01, 0.1, 1],\n",
    "        'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'subsample_freq': [0, 1, 2, 3],\n",
    "        'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'reg_alpha': [0, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
    "        'reg_lambda': [0, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
    "    }\n",
    "    \n",
    "    # Base model\n",
    "    base_model = lgb.LGBMClassifier(\n",
    "        objective='binary',\n",
    "        metric='binary_logloss',\n",
    "        boosting_type='gbdt',\n",
    "        scale_pos_weight=scale_pos_weight_lgb,\n",
    "        random_state=42,\n",
    "        n_jobs=1,  # Set to 1 for each estimator since RandomizedSearchCV parallelizes\n",
    "        verbose=-1\n",
    "    )\n",
    "    \n",
    "    # Custom scorer for F1 (balancing precision and recall)\n",
    "    f1_scorer = make_scorer(f1_score)\n",
    "    \n",
    "    # RandomizedSearchCV with parallelization\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=base_model,\n",
    "        param_distributions=param_distributions,\n",
    "        n_iter=n_iter,\n",
    "        cv=cv,\n",
    "        scoring=f1_scorer,\n",
    "        n_jobs=-1,  # Parallelize across all CPUs\n",
    "        verbose=2 if verbose else 0,\n",
    "        random_state=42,\n",
    "        return_train_score=True\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print('Starting RandomizedSearchCV...')\n",
    "        print('This may take a while with parallelization across all CPUs...\\n')\n",
    "    \n",
    "    # Fit the random search\n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    if verbose:\n",
    "        print('\\n' + '='*60)\n",
    "        print('RANDOM SEARCH COMPLETE')\n",
    "        print('='*60 + '\\n')\n",
    "        print(f'Best score (CV F1): {random_search.best_score_:.4f}')\n",
    "        print(f'\\nBest parameters:')\n",
    "        for param, value in random_search.best_params_.items():\n",
    "            print(f'  {param}: {value}')\n",
    "        \n",
    "        # Show top 5 parameter combinations\n",
    "        results_df = pd.DataFrame(random_search.cv_results_)\n",
    "        results_df = results_df.sort_values('rank_test_score')\n",
    "        \n",
    "        print(f'\\nTop 5 parameter combinations:')\n",
    "        for idx, row in results_df.head(5).iterrows():\n",
    "            print(f'\\n  Rank {int(row[\"rank_test_score\"])}:')\n",
    "            print(f'    Mean CV F1: {row[\"mean_test_score\"]:.4f} (+/- {row[\"std_test_score\"]:.4f})')\n",
    "            print(f'    Mean Fit Time: {row[\"mean_fit_time\"]:.2f}s')\n",
    "    \n",
    "    best_model = random_search.best_estimator_\n",
    "    \n",
    "    if verbose:\n",
    "        print('\\n' + '='*60)\n",
    "        print('Best model ready for predictions!')\n",
    "        print('='*60 + '\\n')\n",
    "    \n",
    "    return best_model, random_search\n",
    "\n",
    "\n",
    "def compare_with_xgboost(y_test, y_test_pred_lgb, y_test_pred_xgb, verbose=True):\n",
    "    \"\"\"Compare LightGBM with XGBoost baseline\"\"\"\n",
    "    if verbose:\n",
    "        print('\\n' + '='*60)\n",
    "        print('COMPARISON: LIGHTGBM VS XGBOOST BASELINE')\n",
    "        print('='*60 + '\\n')\n",
    "    \n",
    "    lgb_test_recall = recall_score(y_test, y_test_pred_lgb)\n",
    "    xgb_test_recall = recall_score(y_test, y_test_pred_xgb)\n",
    "    lgb_test_precision = precision_score(y_test, y_test_pred_lgb)\n",
    "    xgb_test_precision = precision_score(y_test, y_test_pred_xgb)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Test Recall:')\n",
    "        print(f'  LightGBM: {lgb_test_recall:.4f}')\n",
    "        print(f'  XGBoost:  {xgb_test_recall:.4f}')\n",
    "        print(f'  Difference: {(lgb_test_recall - xgb_test_recall):+.4f}')\n",
    "        \n",
    "        print(f'\\nTest Precision:')\n",
    "        print(f'  LightGBM: {lgb_test_precision:.4f}')\n",
    "        print(f'  XGBoost:  {xgb_test_precision:.4f}')\n",
    "        print(f'  Difference: {(lgb_test_precision - xgb_test_precision):+.4f}')\n",
    "        \n",
    "        if lgb_test_recall > xgb_test_recall:\n",
    "            improvement = ((lgb_test_recall - xgb_test_recall) / xgb_test_recall) * 100\n",
    "            print(f'\\n✅ LightGBM achieves {improvement:.2f}% better recall than XGBoost!')\n",
    "        elif lgb_test_recall < xgb_test_recall:\n",
    "            decline = ((xgb_test_recall - lgb_test_recall) / xgb_test_recall) * 100\n",
    "            print(f'\\n⚠️  LightGBM recall is {decline:.2f}% lower than XGBoost')\n",
    "        else:\n",
    "            print(f'\\n➡️  LightGBM and XGBoost achieve the same recall')\n",
    "    \n",
    "    return {\n",
    "        'lgb_recall': lgb_test_recall,\n",
    "        'xgb_recall': xgb_test_recall,\n",
    "        'lgb_precision': lgb_test_precision,\n",
    "        'xgb_precision': xgb_test_precision\n",
    "    }\n",
    "\n",
    "\n",
    "def train_and_evaluate_lightgbm(X_train, y_train, X_test, y_test, \n",
    "                                y_train_pred_xgb, y_test_pred_xgb, \n",
    "                                verbose=True):\n",
    "    \"\"\"Complete LightGBM training and evaluation pipeline with XGBoost features\"\"\"\n",
    "    \n",
    "    # Train model\n",
    "    model = train_lightgbm(X_train, y_train, verbose=verbose)\n",
    "    \n",
    "    # Evaluate model\n",
    "    y_train_pred_lgb, y_test_pred_lgb = evaluate_lightgbm(\n",
    "        model, X_train, y_train, X_test, y_test, verbose=verbose\n",
    "    )\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = get_feature_importance(model, X_train, verbose=verbose)\n",
    "    \n",
    "    # Compare with XGBoost\n",
    "    comparison = compare_with_xgboost(y_test, y_test_pred_lgb, y_test_pred_xgb, verbose=verbose)\n",
    "    \n",
    "    if verbose:\n",
    "        print('\\n' + '='*60)\n",
    "        print('LightGBM training and evaluation complete!')\n",
    "        print('='*60)\n",
    "    \n",
    "    return model, y_train_pred_lgb, y_test_pred_lgb, feature_importance, comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8390afbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING LIGHTGBM - OPTIMIZED FOR RECALL\n",
      "============================================================\n",
      "\n",
      "Class imbalance ratio (neg/pos): 63.47\n",
      "Using scale_pos_weight=63.47 to boost recall\n",
      "\n",
      "Training LightGBM model...\n",
      "Training complete!\n",
      "\n",
      "============================================================\n",
      "EVALUATING LIGHTGBM MODEL\n",
      "============================================================\n",
      "\n",
      "=== TRAIN SET PERFORMANCE ===\n",
      "Recall (fraud class): 0.9277\n",
      "Precision (fraud class): 0.0793\n",
      "F1-Score (fraud class): 0.1461\n",
      "\n",
      "Confusion Matrix (Train):\n",
      "[[511319 104501]\n",
      " [   701   9001]]\n",
      "\n",
      "=== TEST SET PERFORMANCE ===\n",
      "Recall (fraud class): 0.7035\n",
      "Precision (fraud class): 0.0600\n",
      "F1-Score (fraud class): 0.1105\n",
      "\n",
      "Confusion Matrix (Test):\n",
      "[[127218  26738]\n",
      " [   719   1706]]\n",
      "\n",
      "Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.83      0.90    153956\n",
      "         1.0       0.06      0.70      0.11      2425\n",
      "\n",
      "    accuracy                           0.82    156381\n",
      "   macro avg       0.53      0.76      0.51    156381\n",
      "weighted avg       0.98      0.82      0.89    156381\n",
      "\n",
      "\n",
      "Top 15 Most Important Features (LightGBM):\n",
      "merchantName_ordinal                  2702\n",
      "accountNumber                         2530\n",
      "transactionAmount                     2323\n",
      "cardLast4Digits                       2258\n",
      "cardCVV                               2219\n",
      "daysSinceAccountOpen                  2080\n",
      "daysToCurrentExpDate                  1952\n",
      "availableMoney                        1926\n",
      "currentBalance                        1869\n",
      "daysSinceLastAddressChange            1783\n",
      "posEntryMode                           487\n",
      "posConditionCode                       219\n",
      "merchantCategoryCode_entertainment     217\n",
      "merchantCategoryCode_food              152\n",
      "merchantCategoryCode_fastfood          150\n",
      "dtype: int32\n",
      "\n",
      "============================================================\n",
      "COMPARISON: LIGHTGBM VS XGBOOST BASELINE\n",
      "============================================================\n",
      "\n",
      "Test Recall:\n",
      "  LightGBM: 0.7035\n",
      "  XGBoost:  1.0000\n",
      "  Difference: -0.2965\n",
      "\n",
      "Test Precision:\n",
      "  LightGBM: 0.0600\n",
      "  XGBoost:  1.0000\n",
      "  Difference: -0.9400\n",
      "\n",
      "⚠️  LightGBM recall is 29.65% lower than XGBoost\n",
      "\n",
      "============================================================\n",
      "LightGBM training and evaluation complete!\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LGBMClassifier(colsample_bytree=0.8, is_unbalance=True, learning_rate=0.05,\n",
       "                max_depth=8, metric='f1', n_estimators=800, n_jobs=-1,\n",
       "                objective='binary', random_state=42, reg_alpha=0.5,\n",
       "                reg_lambda=0.5, subsample=0.6, subsample_freq=1, verbose=-1),\n",
       " array([0., 0., 1., ..., 0., 0., 0.]),\n",
       " array([0., 0., 0., ..., 0., 0., 0.]),\n",
       " accountNumber                                2530\n",
       " availableMoney                               1926\n",
       " transactionAmount                            2323\n",
       " posEntryMode                                  487\n",
       " posConditionCode                              219\n",
       " cardCVV                                      2219\n",
       " cardLast4Digits                              2258\n",
       " currentBalance                               1869\n",
       " cardPresent                                   108\n",
       " expirationDateKeyInMatch                       14\n",
       " nomerchantCountryCode                          52\n",
       " notransactionType                               8\n",
       " merchantCountryCode_CAN                        13\n",
       " merchantCountryCode_MEX                        15\n",
       " merchantCountryCode_PR                         14\n",
       " merchantCountryCode_US                         28\n",
       " transactionType_ADDRESS_VERIFICATION           17\n",
       " transactionType_PURCHASE                       38\n",
       " transactionType_REVERSAL                       30\n",
       " merchantCategoryCode_airline                   66\n",
       " merchantCategoryCode_auto                      78\n",
       " merchantCategoryCode_cable/phone                0\n",
       " merchantCategoryCode_entertainment            217\n",
       " merchantCategoryCode_fastfood                 150\n",
       " merchantCategoryCode_food                     152\n",
       " merchantCategoryCode_food_delivery              2\n",
       " merchantCategoryCode_fuel                       3\n",
       " merchantCategoryCode_furniture                 55\n",
       " merchantCategoryCode_gym                        0\n",
       " merchantCategoryCode_health                    39\n",
       " merchantCategoryCode_hotels                    73\n",
       " merchantCategoryCode_mobileapps                 1\n",
       " merchantCategoryCode_online_gifts              99\n",
       " merchantCategoryCode_online_retail            109\n",
       " merchantCategoryCode_online_subscriptions       1\n",
       " merchantCategoryCode_personal care             81\n",
       " merchantCategoryCode_rideshare                103\n",
       " merchantCategoryCode_subscriptions             86\n",
       " daysToCurrentExpDate                         1952\n",
       " daysSinceAccountOpen                         2080\n",
       " daysSinceLastAddressChange                   1783\n",
       " merchantName_ordinal                         2702\n",
       " dtype: int32,\n",
       " {'lgb_recall': 0.7035051546391753,\n",
       "  'xgb_recall': 1.0,\n",
       "  'lgb_precision': 0.059977499648432006,\n",
       "  'xgb_precision': 1.0})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, y_pred2, y_test_pred2, _, _ = train_and_evaluate_lightgbm(X_train, y_train, X_test, y_test, y_train, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
